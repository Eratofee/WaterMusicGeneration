{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjIhzBI1HtBompUi/WZRMs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eratofee/WaterMusicGeneration/blob/main/WaterMusicGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install mido pretty_midi\n",
        "!pip install midiutil\n",
        "!apt-get -y -qq update\n",
        "!apt-get -y -qq install fluidsynth fluid-soundfont-gm"
      ],
      "metadata": {
        "id": "D0Hbm1yFHFpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import base64, io, os\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "from IPython.display import HTML, display\n",
        "import pretty_midi"
      ],
      "metadata": {
        "id": "KgJhY0oWHG9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG2ZKSgGhRCc"
      },
      "outputs": [],
      "source": [
        "file_path = \"DataSet_20230217-20231231.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Datum in echtes Zeitformat umwandeln\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"], dayfirst=True)\n",
        "\n",
        "df.head()\n",
        "print(df.columns)\n",
        "# df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- instrument config ----------\n",
        "# GM program numbers are 1..128\n",
        "INSTRUMENTS = [\n",
        "    {\"station\":\"P343\",  \"instrument_name\":\"Violin\",       \"gm_program\":41, \"low\":\"G3\",  \"high\":\"E7\"},\n",
        "    {\"station\":\"P33\",   \"instrument_name\":\"Clarinet\",     \"gm_program\":72, \"low\":\"E3\",  \"high\":\"C7\"},\n",
        "    {\"station\":\"P501\",  \"instrument_name\":\"Cello\",        \"gm_program\":43, \"low\":\"C2\",  \"high\":\"A5\"},  # praxisnah\n",
        "    {\"station\":\"P37\",   \"instrument_name\":\"Trombone\",     \"gm_program\":58, \"low\":\"E2\",  \"high\":\"F5\"},\n",
        "    {\"station\":\"P349\",  \"instrument_name\":\"Contrabass\",   \"gm_program\":44, \"low\":\"E1\",  \"high\":\"C5\"},\n",
        "    {\"station\":\"P38\",   \"instrument_name\":\"Bassoon\",      \"gm_program\":71, \"low\":\"Bb1\", \"high\":\"Eb5\"},\n",
        "    {\"station\":\"P9\",    \"instrument_name\":\"Church Organ\", \"gm_program\":20, \"low\":\"C2\",  \"high\":\"C7\"},\n",
        "    {\"station\":\"P9\",    \"instrument_name\":\"Trumpet\",      \"gm_program\":57, \"low\":\"F#3\", \"high\":\"E6\"},\n",
        "    {\"station\":\"P618\",  \"instrument_name\":\"Oboe\",         \"gm_program\":69, \"low\":\"Bb3\", \"high\":\"A6\"},\n",
        "    {\"station\":\"GN-LRu\",\"instrument_name\":\"Synth Pad 1\",  \"gm_program\":89, \"low\":\"C2\",  \"high\":\"C7\"},\n",
        "]"
      ],
      "metadata": {
        "id": "TRV_1G7pKehS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If \"date\" is a column, use it as x-axis\n",
        "if \"date\" in df.columns:\n",
        "    x = df[\"date\"]\n",
        "    y_df = df.drop(columns=[\"date\"])\n",
        "else:\n",
        "    x = df.index\n",
        "    y_df = df\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Alternative to y_df:\n",
        "y_df_smooth = y_df.rolling(100, min_periods=1).mean()\n",
        "\n",
        "for col in y_df_smooth.columns:\n",
        "    plt.plot(x, y_df_smooth[col], label=col, alpha=0.6)\n",
        "\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Water Level\")\n",
        "plt.title(\"Overview of All Stations\")\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "# Put legend outside for readability\n",
        "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 1.0))\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NxDPh19rqH80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0) Ensure df is in the expected shape: time column or time index ---\n",
        "df2 = df.copy()\n",
        "\n",
        "# If time is in a column called \"date\", use it; otherwise use index as time\n",
        "if \"date\" in df2.columns:\n",
        "    df2[\"date\"] = pd.to_datetime(df2[\"date\"], dayfirst=True, errors=\"coerce\")\n",
        "    df2 = df2.sort_values(\"date\")\n",
        "    time_col = \"date\"\n",
        "else:\n",
        "    # assume index is datetime-like\n",
        "    df2 = df2.copy()\n",
        "    df2.index = pd.to_datetime(df2.index, errors=\"coerce\")\n",
        "    df2 = df2.sort_index()\n",
        "    df2 = df2.reset_index().rename(columns={\"index\": \"date\"})\n",
        "    time_col = \"date\"\n",
        "\n",
        "# --- 1) Station -> Instrument mapping (GM program numbers 1..128) ---\n",
        "# GM reference (common):\n",
        "# Violin 41, Clarinet 72, Cello 43, Trombone 58, Contrabass 44,\n",
        "# Bassoon 71, Church Organ 20, Trumpet 57, Oboe 69, Synth Pad 1 89\n",
        "mapping = pd.DataFrame([\n",
        "    {\"station\": \"P343\",  \"instrument_name\": \"Violin\",        \"midi_program_gm_1_128\": 41},\n",
        "    {\"station\": \"P33\",   \"instrument_name\": \"Clarinet\",      \"midi_program_gm_1_128\": 72},\n",
        "    {\"station\": \"P501\",  \"instrument_name\": \"Cello\",         \"midi_program_gm_1_128\": 43},\n",
        "    {\"station\": \"P37\",   \"instrument_name\": \"Trombone\",      \"midi_program_gm_1_128\": 58},\n",
        "    {\"station\": \"P349\",  \"instrument_name\": \"Contrabass\",    \"midi_program_gm_1_128\": 44},\n",
        "    {\"station\": \"P38\",   \"instrument_name\": \"Bassoon\",       \"midi_program_gm_1_128\": 71},\n",
        "    {\"station\": \"P9\",    \"instrument_name\": \"Church Organ\",  \"midi_program_gm_1_128\": 20},\n",
        "    {\"station\": \"P9\",    \"instrument_name\": \"Trumpet\",       \"midi_program_gm_1_128\": 57},  # duplicate station on purpose\n",
        "    {\"station\": \"P618\",  \"instrument_name\": \"Oboe\",          \"midi_program_gm_1_128\": 69},\n",
        "    {\"station\": \"GN-LRu\",\"instrument_name\": \"Synth Pad 1\",   \"midi_program_gm_1_128\": 89},\n",
        "])\n",
        "\n",
        "# --- 2) Build long/tidy df: time, station, value ---\n",
        "# Keep only stations that appear in mapping\n",
        "stations = mapping[\"station\"].unique().tolist()\n",
        "available = [c for c in stations if c in df2.columns]\n",
        "\n",
        "long_df = df2[[time_col] + available].melt(\n",
        "    id_vars=[time_col],\n",
        "    var_name=\"station\",\n",
        "    value_name=\"value\"\n",
        ").dropna(subset=[\"value\"])\n",
        "\n",
        "# --- 3) Attach instrument info (duplicates on P9 will duplicate rows intentionally) ---\n",
        "assigned_df = long_df.merge(mapping, on=\"station\", how=\"left\")\n",
        "\n",
        "# --- 4) Rename time column to exactly 'time' as requested, and order columns ---\n",
        "assigned_df = assigned_df.rename(columns={time_col: \"time\"})\n",
        "assigned_df = assigned_df[[\"time\", \"station\", \"value\", \"instrument_name\", \"midi_program_gm_1_128\"]]\n",
        "\n",
        "assigned_df.head(10), assigned_df.shape\n"
      ],
      "metadata": {
        "id": "neNCiEN43H0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- helpers: note name <-> MIDI ----------\n",
        "_NOTE2SEMI = {\"C\":0,\"C#\":1,\"Db\":1,\"D\":2,\"D#\":3,\"Eb\":3,\"E\":4,\"F\":5,\"F#\":6,\"Gb\":6,\"G\":7,\"G#\":8,\"Ab\":8,\"A\":9,\"A#\":10,\"Bb\":10,\"B\":11}\n",
        "def note_to_midi(note: str) -> int:\n",
        "    # e.g. \"G3\", \"Bb1\", \"F#5\"\n",
        "    note = note.strip()\n",
        "    if len(note) < 2:\n",
        "        raise ValueError(f\"Bad note: {note}\")\n",
        "    # split pitch class and octave\n",
        "    if note[1] in [\"b\", \"#\"]:\n",
        "        pc = note[:2]\n",
        "        octv = int(note[2:])\n",
        "    else:\n",
        "        pc = note[:1]\n",
        "        octv = int(note[1:])\n",
        "    return 12*(octv + 1) + _NOTE2SEMI[pc]  # MIDI: C4=60\n",
        "\n",
        "def build_dorian_scale(root_midi: int) -> np.ndarray:\n",
        "    # Dorian intervals: 0,2,3,5,7,9,10 (relative to root)\n",
        "    return np.array([root_midi + i for i in [0,2,3,5,7,9,10]], dtype=int)\n",
        "\n",
        "def build_scale_in_range(root_note: str = \"D3\", low: int = 48, high: int = 72) -> np.ndarray:\n",
        "    # Generate D-dorian pitches between [low, high] inclusive.\n",
        "    root = note_to_midi(root_note)\n",
        "    pitches = []\n",
        "    # sweep octaves around the target range\n",
        "    for o in range(-6, 10):\n",
        "        pitches.extend(build_dorian_scale(root + 12*o).tolist())\n",
        "    pitches = np.array(sorted(set(pitches)))\n",
        "    pitches = pitches[(pitches >= low) & (pitches <= high)]\n",
        "    if len(pitches) < 2:\n",
        "        raise ValueError(f\"Scale too small in range low={low}, high={high}.\")\n",
        "    return pitches\n",
        "\n",
        "def quantize_values_to_scale(values: np.ndarray, scale: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Map min(values)->scale[0], max(values)->scale[-1], linearly then quantize to nearest scale index.\"\"\"\n",
        "    v = np.asarray(values, dtype=float)\n",
        "    vmin = np.nanmin(v)\n",
        "    vmax = np.nanmax(v)\n",
        "    if not np.isfinite(vmin) or not np.isfinite(vmax) or abs(vmax - vmin) < 1e-12:\n",
        "        # constant or invalid -> middle pitch\n",
        "        return np.full(len(v), int(scale[len(scale)//2]), dtype=int)\n",
        "    t = (v - vmin) / (vmax - vmin)\n",
        "    idx = np.rint(t * (len(scale)-1)).astype(int)\n",
        "    idx = np.clip(idx, 0, len(scale)-1)\n",
        "    return scale[idx].astype(int)\n",
        "\n",
        "\n",
        "def map_data_to_midi(df: pd.DataFrame,\n",
        "                     instruments=INSTRUMENTS,\n",
        "                     time_col: str = \"date\",\n",
        "                     scale_root: str = \"D3\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns tidy DF with columns:\n",
        "    time, station, instrument_name, gm_program, value, midi_pitch\n",
        "    \"\"\"\n",
        "    d = df.copy()\n",
        "    if time_col in d.columns:\n",
        "        d[time_col] = pd.to_datetime(d[time_col], dayfirst=True, errors=\"coerce\")\n",
        "        d = d.sort_values(time_col)\n",
        "    else:\n",
        "        # assume index is time\n",
        "        d = d.copy()\n",
        "        d.index = pd.to_datetime(d.index, errors=\"coerce\")\n",
        "        d = d.sort_index()\n",
        "        d = d.reset_index().rename(columns={\"index\": time_col})\n",
        "\n",
        "    out_rows = []\n",
        "    for inst in instruments:\n",
        "        st = inst[\"station\"]\n",
        "        if st not in d.columns:\n",
        "            continue\n",
        "\n",
        "        series = d[[time_col, st]].rename(columns={st:\"value\"}).copy()\n",
        "        series[\"value\"] = pd.to_numeric(series[\"value\"], errors=\"coerce\")\n",
        "        # fill missing values (needed to define min/max robustly across the piece)\n",
        "        series[\"value\"] = series[\"value\"].interpolate(limit_direction=\"both\")\n",
        "        series = series.dropna(subset=[\"value\"])\n",
        "\n",
        "        low_m = note_to_midi(inst[\"low\"])\n",
        "        high_m = note_to_midi(inst[\"high\"])\n",
        "        scale = build_scale_in_range(root_note=scale_root, low=low_m, high=high_m)\n",
        "\n",
        "        midi_pitch = quantize_values_to_scale(series[\"value\"].to_numpy(), scale)\n",
        "\n",
        "        tmp = pd.DataFrame({\n",
        "            \"time\": series[time_col].to_numpy(),\n",
        "            \"station\": st,\n",
        "            \"instrument_name\": inst[\"instrument_name\"],\n",
        "            \"gm_program\": inst[\"gm_program\"],\n",
        "            \"value\": series[\"value\"].to_numpy(),\n",
        "            \"midi_pitch\": midi_pitch\n",
        "        })\n",
        "        out_rows.append(tmp)\n",
        "\n",
        "    mapped = pd.concat(out_rows, ignore_index=True)\n",
        "    # track label for notation tools\n",
        "    mapped[\"track_name\"] = mapped[\"instrument_name\"] + \" (\" + mapped[\"station\"] + \")\"\n",
        "    return mapped\n",
        "\n",
        "# ---------- rhythm + MIDI rendering ----------\n",
        "def render_rhythmic_midi(mapped_df: pd.DataFrame,\n",
        "                         bars: int = 64,\n",
        "                         tempo_bpm: int = 80,\n",
        "                         time_signature=(4,4),\n",
        "                         quarter_weight: float = 0.55,\n",
        "                         seed: int = 7,\n",
        "                         ticks_per_beat: int = 480,\n",
        "                         outfile: str = \"water_piece_64bars.mid\"):\n",
        "    \"\"\"\n",
        "    Creates a GM MIDI file with named tracks. Rhythm grid is 1/8 notes, durations allowed:\n",
        "    eighth (0.5), quarter (1), half (2), whole (4) beats.\n",
        "    Highest probability for quarter note by default.\n",
        "    \"\"\"\n",
        "    # lazy import / install hint\n",
        "    try:\n",
        "        import mido\n",
        "        from mido import Message, MetaMessage\n",
        "    except Exception as e:\n",
        "        raise ImportError(\"Please install mido first (in Colab: !pip -q install mido).\") from e\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    beats_per_bar = time_signature[0] * (4 / time_signature[1])\n",
        "    total_beats = int(round(bars * beats_per_bar))\n",
        "\n",
        "    # Base grid: eighth notes => 2 steps per beat\n",
        "    steps_per_beat = 2\n",
        "    total_steps = total_beats * steps_per_beat  # each step = 1/8\n",
        "    step_len_beats = 1 / steps_per_beat\n",
        "\n",
        "    # Allowed durations in steps (1=8th, 2=quarter, 4=half, 8=whole)\n",
        "    dur_steps = np.array([1, 2, 4, 8], dtype=int)\n",
        "\n",
        "    # Weights: quarter highest by requirement\n",
        "    # You can tweak these weights if you want “more long notes”.\n",
        "    # quarter_weight controls the quarter note dominance.\n",
        "    # Distribute remaining probability over other values:\n",
        "    rem = 1.0 - quarter_weight\n",
        "    weights = np.array([rem*0.25, quarter_weight, rem*0.40, rem*0.35], dtype=float)\n",
        "    weights = weights / weights.sum()\n",
        "\n",
        "    # Resample each track's pitch stream to exactly total_steps points\n",
        "    tracks = []\n",
        "    for track_name, g in mapped_df.groupby(\"track_name\", sort=False):\n",
        "        g = g.sort_values(\"time\")\n",
        "        pitches = g[\"midi_pitch\"].to_numpy().astype(int)\n",
        "\n",
        "        # If data very long: compress via index-space interpolation (keeps global contour)\n",
        "        if len(pitches) == 0:\n",
        "            continue\n",
        "        x_old = np.linspace(0, 1, num=len(pitches))\n",
        "        x_new = np.linspace(0, 1, num=total_steps)\n",
        "        pitches_rs = np.interp(x_new, x_old, pitches).round().astype(int)\n",
        "\n",
        "        tracks.append({\n",
        "            \"track_name\": track_name,\n",
        "            \"gm_program\": int(g[\"gm_program\"].iloc[0]),\n",
        "            \"pitches_steps\": pitches_rs\n",
        "        })\n",
        "\n",
        "    # Helper: turn stepwise pitches into note events with allowed durations\n",
        "    def steps_to_events(pitches_steps: np.ndarray):\n",
        "        # run-length encode identical pitches\n",
        "        events = []\n",
        "        i = 0\n",
        "        while i < len(pitches_steps):\n",
        "            p = int(pitches_steps[i])\n",
        "            j = i + 1\n",
        "            while j < len(pitches_steps) and int(pitches_steps[j]) == p:\n",
        "                j += 1\n",
        "            run_len = j - i  # in steps\n",
        "\n",
        "            # split this run into allowed dur_steps with probabilistic preference\n",
        "            remaining = run_len\n",
        "            while remaining > 0:\n",
        "                possible = dur_steps[dur_steps <= remaining]\n",
        "                w = weights[:len(possible)].copy()\n",
        "                w = w / w.sum()\n",
        "                d = int(rng.choice(possible, p=w))\n",
        "                events.append((p, d))\n",
        "                remaining -= d\n",
        "\n",
        "            i = j\n",
        "        return events\n",
        "\n",
        "    # Build MIDI\n",
        "    mid = mido.MidiFile(type=1, ticks_per_beat=ticks_per_beat)\n",
        "\n",
        "    # Global meta track\n",
        "    meta = mido.MidiTrack()\n",
        "    mid.tracks.append(meta)\n",
        "    meta.append(MetaMessage(\"track_name\", name=\"Score\", time=0))\n",
        "    meta.append(MetaMessage(\"time_signature\", numerator=time_signature[0], denominator=time_signature[1], time=0))\n",
        "    meta.append(MetaMessage(\"set_tempo\", tempo=mido.bpm2tempo(tempo_bpm), time=0))\n",
        "\n",
        "    # One track per instrument\n",
        "    for tr in tracks:\n",
        "        t = mido.MidiTrack()\n",
        "        mid.tracks.append(t)\n",
        "        t.append(MetaMessage(\"track_name\", name=tr[\"track_name\"], time=0))\n",
        "        # GM program change: mido uses 0..127 internally\n",
        "        t.append(Message(\"program_change\", program=tr[\"gm_program\"]-1, time=0, channel=0))\n",
        "\n",
        "        events = steps_to_events(tr[\"pitches_steps\"])\n",
        "\n",
        "        # Write note events with delta-times in ticks\n",
        "        # Each step is an eighth note => ticks = ticks_per_beat/2\n",
        "        step_ticks = int(ticks_per_beat / steps_per_beat)\n",
        "        velocity = 90\n",
        "\n",
        "        for pitch, d_steps in events:\n",
        "            dur_ticks = d_steps * step_ticks\n",
        "            t.append(Message(\"note_on\", note=int(pitch), velocity=velocity, time=0, channel=0))\n",
        "            t.append(Message(\"note_off\", note=int(pitch), velocity=0, time=dur_ticks, channel=0))\n",
        "\n",
        "    mid.save(outfile)\n",
        "    return outfile\n"
      ],
      "metadata": {
        "id": "bqtX82jd85ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- Example usage ----------\n",
        "mapped = map_data_to_midi(df, scale_root=\"D3\")\n",
        "midi_path = render_rhythmic_midi(mapped, bars=64, tempo_bpm=80, time_signature=(4,4))\n",
        "print(\"Wrote:\", midi_path)"
      ],
      "metadata": {
        "id": "-a_TBjlU889o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def midi_to_partiture_image(\n",
        "    midi_path: str,\n",
        "    bars: int = 64,\n",
        "    time_signature=(4, 4),\n",
        "    outfile_png: str | None = None,\n",
        "    figsize=(20, 6),\n",
        "    min_note_len_beats=0.125,   # discard micro-blips\n",
        "    show_pitch_range=False      # if True, shows pitch min/max per track on the left\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a one-line (all bars in one row on x-axis) 'partiture image' from a MIDI file.\n",
        "    Each MIDI track becomes one staff-row (voice). Notes are drawn as horizontal bars.\n",
        "\n",
        "    X-axis: beats (optionally labeled by bar numbers)\n",
        "    Y-axis: voices (tracks)\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Import mido lazily (clear error message if missing) ---\n",
        "    try:\n",
        "        import mido\n",
        "    except Exception as e:\n",
        "        raise ImportError(\"mido is required. Install via: pip install mido (Colab: !pip -q install mido)\") from e\n",
        "\n",
        "    mid = mido.MidiFile(midi_path)\n",
        "    tpq = mid.ticks_per_beat\n",
        "\n",
        "    beats_per_bar = time_signature[0] * (4 / time_signature[1])\n",
        "    total_beats = bars * beats_per_bar\n",
        "\n",
        "    # --- Helper: extract track name ---\n",
        "    def get_track_name(track, default):\n",
        "        for msg in track:\n",
        "            if msg.type == \"track_name\":\n",
        "                return msg.name\n",
        "        return default\n",
        "\n",
        "    # --- Parse notes per track into (start_beat, duration_beats, pitch) ---\n",
        "    tracks_notes = []\n",
        "    for ti, track in enumerate(mid.tracks):\n",
        "        name = get_track_name(track, f\"Track {ti}\")\n",
        "\n",
        "        # We typically skip the global meta track (\"Score\") if it has no notes\n",
        "        abs_ticks = 0\n",
        "        on = {}  # (channel, note) -> start_tick\n",
        "        notes = []\n",
        "\n",
        "        for msg in track:\n",
        "            abs_ticks += msg.time\n",
        "            if msg.type == \"note_on\" and msg.velocity > 0:\n",
        "                on[(getattr(msg, \"channel\", 0), msg.note)] = abs_ticks\n",
        "            elif (msg.type == \"note_off\") or (msg.type == \"note_on\" and msg.velocity == 0):\n",
        "                key = (getattr(msg, \"channel\", 0), msg.note)\n",
        "                if key in on:\n",
        "                    start = on.pop(key)\n",
        "                    dur = abs_ticks - start\n",
        "                    start_beat = start / tpq\n",
        "                    dur_beats = dur / tpq\n",
        "                    if dur_beats >= min_note_len_beats:\n",
        "                        notes.append((start_beat, dur_beats, int(msg.note)))\n",
        "\n",
        "        if len(notes) > 0:\n",
        "            tracks_notes.append({\"name\": name, \"notes\": notes})\n",
        "\n",
        "    if not tracks_notes:\n",
        "        raise ValueError(\"No note events found in MIDI (or all were filtered out).\")\n",
        "\n",
        "    # --- Build plot ---\n",
        "    n = len(tracks_notes)\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Y layout: one band per track\n",
        "    row_height = 0.7\n",
        "    y_positions = np.arange(n)[::-1]  # top to bottom\n",
        "    y_map = {i: y_positions[i] for i in range(n)}\n",
        "\n",
        "    # For each track, compress pitch to a thin band? -> We keep pitch-variation *within the row* by vertical offset.\n",
        "    # But you asked: \"one row per voice\", so pitch should not create extra rows.\n",
        "    # Solution: draw pitch as slight vertical offset within the same row band.\n",
        "    # Normalize pitch per track to [0..row_height] inside that row.\n",
        "    for i, tr in enumerate(tracks_notes):\n",
        "        notes = tr[\"notes\"]\n",
        "        pitches = np.array([p for (_, _, p) in notes], dtype=float)\n",
        "        pmin, pmax = pitches.min(), pitches.max()\n",
        "        if pmax - pmin < 1e-9:\n",
        "            norm = lambda p: 0.35  # constant mid\n",
        "        else:\n",
        "            norm = lambda p: 0.1 + 0.6 * ((p - pmin) / (pmax - pmin))\n",
        "\n",
        "        y0 = y_map[i]\n",
        "\n",
        "        for (start_b, dur_b, pitch) in notes:\n",
        "            # Clip to desired window (0..total_beats)\n",
        "            if start_b > total_beats:\n",
        "                continue\n",
        "            end_b = min(start_b + dur_b, total_beats)\n",
        "            dur_draw = max(0.0, end_b - start_b)\n",
        "            if dur_draw <= 0:\n",
        "                continue\n",
        "\n",
        "            y = y0 + norm(pitch) * row_height\n",
        "            ax.broken_barh([(start_b, dur_draw)], (y, 0.12), linewidth=0)\n",
        "\n",
        "        # Track label on left\n",
        "        label = tr[\"name\"]\n",
        "        if show_pitch_range:\n",
        "            label += f\"  [{int(pmin)}–{int(pmax)}]\"\n",
        "        ax.text(-0.5, y0 + 0.35, label, va=\"center\", ha=\"right\")\n",
        "\n",
        "    # X axis: bar grid\n",
        "    bar_ticks = np.arange(0, total_beats + 1e-9, beats_per_bar)\n",
        "    for bt in bar_ticks:\n",
        "        ax.axvline(bt, linewidth=0.5, alpha=0.3)\n",
        "\n",
        "    ax.set_xlim(0, total_beats)\n",
        "    ax.set_ylim(-0.5, n - 0.5 + row_height)\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    ax.set_xlabel(\"Time (beats) — all bars in one line\")\n",
        "    ax.set_title(\"MIDI Note Partiture (one row per voice, all bars in one line)\")\n",
        "\n",
        "    # Optional: label every 4 bars to keep it readable\n",
        "    label_every = 4\n",
        "    xt = bar_ticks[::label_every]\n",
        "    ax.set_xticks(xt)\n",
        "    ax.set_xticklabels([f\"{int(b/ beats_per_bar) + 1}\" for b in xt])  # bar numbers starting at 1\n",
        "    ax.set_xlabel(f\"Bar number (every {label_every} bars)\")\n",
        "\n",
        "    ax.grid(False)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if outfile_png:\n",
        "        fig.savefig(outfile_png, dpi=200, bbox_inches=\"tight\")\n",
        "    return fig, ax\n"
      ],
      "metadata": {
        "id": "5izRVlc4LUPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = midi_to_partiture_image(\n",
        "    \"water_piece_64bars.mid\",\n",
        "    bars=64,\n",
        "    time_signature=(4,4),\n",
        "    outfile_png=\"partiture.png\",\n",
        "    figsize=(24, 6)\n",
        ")\n"
      ],
      "metadata": {
        "id": "I5oj1OSTLZvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, io, base64, subprocess, tempfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def _find_soundfont():\n",
        "    candidates = [\n",
        "        \"/usr/share/sounds/sf2/FluidR3_GM.sf2\",\n",
        "        \"/usr/share/sounds/sf2/FluidR3_GS.sf2\",\n",
        "    ]\n",
        "    for p in candidates:\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    raise FileNotFoundError(\"GM SoundFont not found. Install: apt-get install fluid-soundfont-gm\")\n",
        "\n",
        "def _fig_to_base64_png(fig, dpi=160):\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format=\"png\", dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "    buf.seek(0)\n",
        "    return base64.b64encode(buf.read()).decode(\"ascii\")\n",
        "\n",
        "def _file_to_base64(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        return base64.b64encode(f.read()).decode(\"ascii\")\n",
        "\n",
        "def midi_to_wav_bytes_via_cli(midi_path, sr=44100):\n",
        "    sf2 = _find_soundfont()\n",
        "    with tempfile.TemporaryDirectory() as td:\n",
        "        wav_path = os.path.join(td, \"out.wav\")\n",
        "        # fluidsynth -ni <soundfont> <midi> -F <wav> -r <sr>\n",
        "        cmd = [\"fluidsynth\", \"-ni\", sf2, midi_path, \"-F\", wav_path, \"-r\", str(sr)]\n",
        "        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        with open(wav_path, \"rb\") as f:\n",
        "            wav_bytes = f.read()\n",
        "    return wav_bytes\n",
        "\n",
        "def build_combined_figure(mapped_df, bars=64, time_signature=(4,4),\n",
        "                          top_tracks=(\"Violin (P343)\", \"Cello (P501)\"),\n",
        "                          second_tracks=(\"Clarinet (P33)\", \"Trombone (P37)\"),\n",
        "                          figsize=(20,8)):\n",
        "    beats_per_bar = time_signature[0] * (4 / time_signature[1])\n",
        "    total_beats = bars * beats_per_bar\n",
        "    beats_grid = np.linspace(0, total_beats, num=4000)\n",
        "\n",
        "    def track_curve(track_name, col=\"value\"):\n",
        "        g = mapped_df[mapped_df[\"track_name\"] == track_name].sort_values(\"time\")\n",
        "        if len(g) < 2:\n",
        "            return np.full_like(beats_grid, np.nan, dtype=float)\n",
        "        y = g[col].to_numpy(dtype=float)\n",
        "        x_old = np.linspace(0, total_beats, num=len(y))\n",
        "        return np.interp(beats_grid, x_old, y)\n",
        "\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    gs = fig.add_gridspec(3, 1, height_ratios=[1, 1, 1.4], hspace=0.15)\n",
        "    ax1 = fig.add_subplot(gs[0,0])\n",
        "    ax2 = fig.add_subplot(gs[1,0], sharex=ax1)\n",
        "    ax3 = fig.add_subplot(gs[2,0], sharex=ax1)\n",
        "\n",
        "    # Top graph\n",
        "    for tname in top_tracks:\n",
        "        ax1.plot(beats_grid, track_curve(tname, \"value\"), label=tname, alpha=0.9)\n",
        "    ax1.set_ylabel(\"Value\"); ax1.grid(True, alpha=0.25); ax1.legend(loc=\"upper right\")\n",
        "\n",
        "    # Second graph\n",
        "    for tname in second_tracks:\n",
        "        ax2.plot(beats_grid, track_curve(tname, \"value\"), label=tname, alpha=0.9)\n",
        "    ax2.set_ylabel(\"Value\"); ax2.grid(True, alpha=0.25); ax2.legend(loc=\"upper right\")\n",
        "\n",
        "    # Partiture (one row per track)\n",
        "    tracks = list(mapped_df[\"track_name\"].dropna().unique())\n",
        "    row_height = 0.8\n",
        "    y_positions = np.arange(len(tracks))[::-1]\n",
        "    steps_per_beat = 2  # eighth grid\n",
        "    total_steps = int(total_beats * steps_per_beat)\n",
        "    step_len = 1.0 / steps_per_beat\n",
        "\n",
        "    for i, tname in enumerate(tracks):\n",
        "        g = mapped_df[mapped_df[\"track_name\"] == tname].sort_values(\"time\")\n",
        "        if len(g) == 0:\n",
        "            continue\n",
        "        pitches = g[\"midi_pitch\"].to_numpy(dtype=float)\n",
        "        x_old = np.linspace(0, 1, num=len(pitches))\n",
        "        x_new = np.linspace(0, 1, num=total_steps)\n",
        "        p_steps = np.interp(x_new, x_old, pitches).round().astype(int)\n",
        "\n",
        "        y0 = y_positions[i]\n",
        "        j = 0\n",
        "        while j < len(p_steps):\n",
        "            p = p_steps[j]\n",
        "            k = j + 1\n",
        "            while k < len(p_steps) and p_steps[k] == p:\n",
        "                k += 1\n",
        "            ax3.broken_barh([(j*step_len, (k-j)*step_len)], (y0, 0.18), linewidth=0)\n",
        "            j = k\n",
        "\n",
        "        ax3.text(-0.8, y0 + 0.08, tname, va=\"center\", ha=\"right\")\n",
        "\n",
        "    bar_ticks = np.arange(0, total_beats + 1e-9, beats_per_bar)\n",
        "    for bt in bar_ticks:\n",
        "        ax3.axvline(bt, linewidth=0.5, alpha=0.25)\n",
        "\n",
        "    ax3.set_xlim(0, total_beats)\n",
        "    ax3.set_ylim(-0.5, len(tracks)-0.5 + row_height)\n",
        "    ax3.set_yticks([])\n",
        "    label_every = 4\n",
        "    xt = bar_ticks[::label_every]\n",
        "    ax3.set_xticks(xt)\n",
        "    ax3.set_xticklabels([str(int(b/beats_per_bar)+1) for b in xt])\n",
        "    ax3.set_xlabel(f\"Bar number (every {label_every} bars)\")\n",
        "    ax3.set_title(\"Partiture (one row per voice)\")\n",
        "    return fig\n",
        "\n",
        "def player_with_red_line(midi_path, mapped_df, bars=64, time_signature=(4,4),\n",
        "                         top_tracks=(\"Violin (P343)\", \"Cello (P501)\"),\n",
        "                         second_tracks=(\"Clarinet (P33)\", \"Trombone (P37)\"),\n",
        "                         width_px=1400):\n",
        "    # 1) MIDI -> WAV via fluidsynth CLI\n",
        "    wav_bytes = midi_to_wav_bytes_via_cli(midi_path, sr=44100)\n",
        "    wav_b64 = base64.b64encode(wav_bytes).decode(\"ascii\")\n",
        "\n",
        "    # 2) Build combined figure -> PNG base64\n",
        "    fig = build_combined_figure(mapped_df, bars=bars, time_signature=time_signature,\n",
        "                                top_tracks=top_tracks, second_tracks=second_tracks)\n",
        "    img_b64 = _fig_to_base64_png(fig, dpi=160)\n",
        "\n",
        "    # 3) HTML player + synced playhead\n",
        "    html = f\"\"\"\n",
        "    <div style=\"width:{width_px}px; position:relative; font-family:sans-serif;\">\n",
        "      <audio id=\"audio\" controls style=\"width:{width_px}px;\">\n",
        "        <source src=\"data:audio/wav;base64,{wav_b64}\" type=\"audio/wav\">\n",
        "      </audio>\n",
        "      <div style=\"width:{width_px}px; position:relative; margin-top:8px;\">\n",
        "        <img id=\"scoreimg\" src=\"data:image/png;base64,{img_b64}\" style=\"width:{width_px}px; display:block;\"/>\n",
        "        <div id=\"playhead\" style=\"position:absolute; top:0; left:0; width:2px; height:100%;\n",
        "             background:red; opacity:0.9; pointer-events:none;\"></div>\n",
        "      </div>\n",
        "    </div>\n",
        "    <script>\n",
        "      const audio = document.getElementById('audio');\n",
        "      const img = document.getElementById('scoreimg');\n",
        "      const head = document.getElementById('playhead');\n",
        "\n",
        "      function tick(){{\n",
        "        const dur = audio.duration || 1;\n",
        "        const t = audio.currentTime || 0;\n",
        "        const frac = Math.min(1, Math.max(0, t / dur));\n",
        "        const w = img.getBoundingClientRect().width;\n",
        "        head.style.left = (frac * w) + \"px\";\n",
        "        requestAnimationFrame(tick);\n",
        "      }}\n",
        "      img.onload = ()=>requestAnimationFrame(tick);\n",
        "    </script>\n",
        "    \"\"\"\n",
        "    display(HTML(html))\n",
        "\n",
        "# Usage:\n",
        "# player_with_red_line(midi_path, mapped, bars=64)"
      ],
      "metadata": {
        "id": "d8-2PYATNW2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "player_with_red_line(midi_path, mapped, bars=64)"
      ],
      "metadata": {
        "id": "g7kTtmkZN0LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sXu1GhuP9-uc"
      }
    }
  ]
}